% $Id$

\documentclass{elsart}
\usepackage{mathptmx}
%\usepackage{courier}
\usepackage{graphicx}

\begin{document}

\begin{frontmatter}

\title{A Software Development Metaphor for Developing Semi-dynamic Web Sites through Declarative Specifications}

\author{Diomidis Spinellis, Vassilios Karakoidas and Damianos Chatziantoniou}

\address{Department of Management Science and Technology, Athens University of Economics and Business, Greece email:\{dds, bkarak, damianos\}@aueb.gr}

\maketitle

\begin{abstract}
Traditionally, the realization of Web sites involves either
static content developed using web authoring tools or dynamic
content delivered by a database driven front-end,
where the structured content is organized
in a relational schema and dynamically generated.
The limitations of statically-authored web pages are easy to discern and
for a number of applications, the use of a database
introduces a level of additional complexity that
makes the choice a part of the problem space rather than the solution space.
Based on the distributed software development approach, we present a methodology suitable for managing 
middle-sized semi-dynamic web sites. The technological dimensions of this
approach are well-known open source technologies, such as version control systems
and {\sc xml} transformation tools. The prototype framework we developed for the testing of the proposed methodology
is also demonstrated, along with interesting conclusions from our framework acceptance.
\end{abstract}

\subsection*{Keywords}
Web site development, semi-dynamic web sites, {\sc xml}, make, {\sc BibTeX}, {\sc xslt}, {\sc X-Schema (xsd)}, {\sc html}.

\end{frontmatter}

\section{Introduction}
\label{sec:intro}
Traditionally, the realization of Web sites involves either
static content developed using web authoring tools like
icrosoft's Front Page and DreamWeaver, or dynamic
content delivered by a data-oriented front-end,
where the structured content is stored in a relational database 
and dynamically generated on the fly.
When our group faced a series of problems with both the above approaches,
we decided to explore ideas for a radically different
implementation style, based on the declarative specification
of all the site's elements.

The limitations of statically-authored web pages are easy to discern.
The content is authored in an loosely organized manner, manually updated web pages, and, as a result,
can be inconsistent in both structure and presentation.
While the use of cascading style sheets can help one obtain a
consistent look, their use still requires discipline.
Moreover, the authored pages remain loosely structured and the resulting
site can be difficult to modify and reorganize. In addition, the information is often duplicated
among hard coded web pages.
Furthermore, the static authoring model often imposes a centralized
management and maintenance style;
all additions and changes have to go through a single person,
creating a bottleneck, often leading to outdated content.

Adopting a database driven approach solves
the aforementioned problems.
Separating the source data from its (dynamically generated)
marked-up version ({\sc html} code) leads to a consistent
yet flexible generation of web pages.
In addition, the database's relational model imposes
its structure on the data being stored.
Finally, a database back-end allows concurrent updates by
different users.

However, for a number of applications, the use of a database
introduces a level of additional complexity that
makes the choice a part of
the problem space rather than the solution space.
A database-driven web site requires the implementation of a
front-side interface to transform the web site's content into
{\sc html} code, and a back-end interface to allow stakeholders
enter, review, and update data.
The back-end client interface typically requires setting up
and maintaining appropriate access permissions.
These may need to be integrated into an organization wide authentication 
facility, or operated under a specific security policy.
In the second case, procedures for setting up passwords,
resetting them, and revoking them need to be established and followed.
A properly running database also requires a skilled database
administrator to install it, maintain it, organize backups,
and perform modifications to the database schema.
In addition, marked-up content is generated by a front-end
program accessing the database, therefore the front-end and the database
must be extremely secure and robust \cite{VG01}, running on a $24 \times 7$ basis.
The front-end, being an executable program working on
untrusted data (the web page requests) can become the target of
malicious attacks,
and must therefore be inspected and audited to ensure its robustness \cite{YHDM04}.

To minimize the risk of an attack against the database
(that would jeopardize the organization's data)
the database server has to be installed on a machine separate
from the web server, behind a properly configured firewall.
Finally, the extraction of content from a database often
induces the web site's designers and stakeholders to adopt a
query-style interface.
Such an interface is typically less usable than browsable web pages,
and the served content is often ignored by search engines,
leading to reduced visibility
of the (meticulously structured) content \cite{DEEP_WEB,JP04}.

All in all, a database-driven approach appears to be suitable
only for those with ample resources to justify the full
development and appropriate maintenance of a sophisticated infrastructure.
Elmasri and Navathe have identified the implications that are created when an organization
adopts such an approach \cite{EN00}. They clarify that an organization should avoid
using a database system, if they cannot afford the overhead cost for:

\begin{itemize}
	\item High initial investment in hardware, software and training

	\item Generality of the {\sc dbms} for the definition and processing of data

	\item Provision of security, concurrency control, recovery, integrity functions
\end{itemize}

In addition, they note that it is desirable to use a straight-forward regular file solution when the following criteria are met.

\begin{itemize}
	\item The database and applications are simple, well defined and not expected to change

	\item Multiple user access to data is not required
\end{itemize}

Having faced the problems we described above within our organization,
we reasoned that a fresh approach was needed to tackle them.
Specifically,
in an internal effort to develop and maintain our group's web site
we had already abandoned the ad-hoc authoring tool--based approach
because it led to an inconsistent look and stale content,
while the maintenance of a subsequent database-driven
design approach was proving intractable for the resources that
our group could afford.
In the following sections we describe the methodology we developed
for implementing semi-dynamic web sites,
illustrate it by means
of a case study covering the requirements, design, and implementation,
and discuss the lessons we learned.

\section{Related Work}
\label{sec:related}

During the last decade, the World Wide Web became a popular platform for many {\sc it} applications.
Researchers and companies, in their attempt to make robust and easily maintainable web applications,
developed many frameworks and tools to formalize and make efficient the creation and maintenance process.

Fraternali \cite{FRA99} analyzed and studied the perspectives of
web development. He pointed out, that the development process has
five distinct perspectives: (1) {\em process}, (2) {\em models, languages and notation}, (3) {\em reuse}, (4) {\em architecture} and (5) {\em usability}.
These include elements like {\em requirements}, {\em  prototyping and validation}, {\em evolution and maintenance}, that apply in
software engineering scientific areas, and other aesthetic elements like {\em visual quality} and {\em degree of proactivity}.
As we will see in section \ref{sec:requirements}, in our work we took into consideration all the above perspectives and we introduced a few more, like {\em openness}
and {\em observability}.
As a plus, in our methodology we implement the system taking into account the organizational aspect of maintainability of content and the underlying application.

In addition, Fraternali categorizes relevant software tools into six individual categories: (1) {\em visual editors and site managers},
 (2) {\em web-enabled hypermedia authoring tools}, (3) {\em web-{\sc dbpl} (database programming languages) integrators}, (4) {\em web form editors, report writers,
 and database publishing wizards},
 (5) {\em multiparadigm tools} and (6) {\em model-driven application generators} \cite{FRA99}. These tools are both commercial and open-source and they are widely used.
 In section \ref{subsec:key-tech}, we will see that in our case we chose open source tools to satisfy the {\em openness} non-functional requirement.
 
Many research groups also designed and implemented frameworks to organize the web development process. These frameworks addressed issues
such as {\em lifecycle coverage}, {\em process automation}, {\em modeling abstractions}, {\em reuse and components}. The most popular implementations are 
Araneus \cite{MAM03}, 
Autoweb \cite{FP00} and Strudel \cite{FFLS00}. Many are extended to enhance their abilities. For example, the Strudel
framework introduced a declarative query language, called StruQL that was later extended by FunStruQL \cite{FST99}. Another interesting
framework is WebJinn \cite{KL03}. This framework tries to resolve the crosscutting concerns in web development.

All the above architectures are introducing development methodologies and languages/scriptlets to achieve their goals. For example, Strudel with StruQL
and Araneus with homer. Our goal was to create a system
that would depend on well-known technologies like {\sc xml} and {\sc cvs}.

For a successful high-level design of a web application,
a visual notation is often introduced to specify composition and navigation features in
hypertext applications.
As an example, {\sc WebML} \cite{CFB00,WEBML} is a language based on standards like Entity-Relationship model {\sc (erd)} and {\sc uml} \cite{UML}.
{\sc WebML} has been proposed in the context of {\sc W3I3} Esprit project.
Araneus also defined a logical model known as the Araneus Logical Model \cite{MAM03}.
These higher level abstractions are well defined and very useful for the overall design of a web site, but are based on custom technologies.
In our case we based our semantics on {\sc xml} to provide a common data abstraction layer.

Some of the characteristics of our approach have also been implemented by
Jenkins \cite{Jen04} in what he terms
offline programmatic generation of web pages.
Our approach however differs in that we use a declarative specification,
rather than imperative code for creating the static content.

Similarities also exist in the context of content management systems ({\sc cms}) and enterprise content management ({\sc ecm}).
{\sc cms} support the creation, management, distribution, publishing, and discovery of corporate information, where {\sc ecm} 
involves a broader range of organizational information. Leading {\sc ecm} providers include Interwoven, 
Vignette (www.vignette.com), Documentum and others. In those systems, similar motivation to ours 
leads the research and analogous goals are set \cite{KW05,DOC05}. For example, the infeasibility of static HTML pages for modern 
web sites and the problems of database-driven approach are also mentioned in \cite{KW05}. Quite recently, {\sc cms}s based solely on 
open standards architecture have been developed, utilizing methodologies similar to those described here \cite{OS05}. However,
 we do not propose a content management system but an approach for managing content based on established, widely deployed 
and tested software configuration management tools. Our approach can be readily adopted by teams already using {\sc cms}. 
Our use of industry-standard data formats and tools provides us with proven scalability (the same tools have been used 
to manage multi-million line software projects) and flexibility (the tools are used as building blocks and a freely 
configurable environment). The downside of our approach is that many features that come out of the box in {\sc cms}s need 
to be explicitly tailored.

\section{Methodology}
\label{sec:meth}
Our proposed methodology for developing semi-dynamic web sites uses as
its metaphor the distributed software development activity, as popularized
by many high-visibility distributed open-source development projects,
such as Eclipse \cite{GB04}, Mozilla, and Free{\sc bsd}.
Using a metaphor as a guiding principle for a development activity
is a practice we adopted from extreme programming \cite{Bec00}.
The methodology can be summarized in a few key points.

\begin{itemize}
\item Structure the site's data as {\sc xml} files,
or another declarative formalism.
Content developers create and edit these to modify the site's contents.
\item Use {\sc xml} schemas and corresponding tools to validate the content.
\item Exploit modular {\sc xhtml} \cite{W3C_MODULAR_XHTML} to create schemas for rich
data elements.
These can be used to allow the inclusion of specific {\sc xhtml} tags
in the site's {\sc xml} files, without having to reinvent {\sc html}
from scratch.
\item Adopt a version control system for distributing the content,
templates, transformation, and schemas across content developers and
administrators, synchronizing updates, and keeping a record of the project's history.
(Thomas and Hunt \cite{HT00} aptly compare a version control system
with a global undo command with unlimited undo levels.)
\item Create the site's look and feel in {\sc xhtml} format
through {\sc xslt}  transformations \cite{HM01}.
\item Express verification and building dependencies among
data elements and schemas through {\em make} \cite{MAKE} or {\em ant}
\cite{JAKARTA_ANT} rules.
\item Make the local generation of the site's content the default
building option to allow developers and administrators to verify
the results of  their work, before putting them online.
\item Use existing mechanisms and tools, such as the secure session shell,
public key authentication, and group membership permissions to implement
authentication and authorization policies.
\item Have an instance of the project on a centralized server, kept up to
date through the version control system, as the source to generate and export
the definitive version web site's contents.
\end{itemize}

One can easily discern from the above points the similarities of our
approach with distributed software development.
A version control system is used to distribute the artifacts among
developers in their current form, and a building tool, such as {\em make}
or {\em ant} guides the building process.
Developers edit and build their work locally, and commit it to a central server when ready.
The repository on the central server is the source for creating the end-result
of the product, through a -usually- {\em daily build} process.
As we will see in later section, in our case the developers work with {\sc xml} and {\sc xslt}
files, instead of programming language source code files, and the {\sc build} process can be called exclusively as a part of
the self-reviewing process.

Our proposal also covers the issue of \textit{efficient replication management}, for almost all the available types of Content Delivery Networks ({\sc cdn}). Our version control system centric approach, simplifies the integration of Server Triggered Replication Strategies \cite{SSPS04}, where the content can be generated in each replica. On the other hand, an organization can apply a static content {\sc html} replication strategy, since all the web site is generated as plain {\sc html} pages.

In order to provide a more detailed comparison between our proposed approach and other commercial or open source {\sc cms's} we 
created a comparison table based on criteria found in CMSMatrix web site \cite{PBC05}. The feature matrix is illustrated in Figure \ref{tbl:cms-matrix}.

\begin{figure}
\begin{center}
\begin{tabular}{c c | c c}
\hline
\multicolumn{2}{c|}{System Requirements} & \multicolumn{2}{c}{Security}\\
\hline
Application Server & None & Audit Trail & Yes\\
Database & None & Content Approval & Yes\\
License & Free/Open Source & Login History & Yes\\
Operating System & {\sc unix}, Win32 & Versioning & Yes\\
Pro/ing Language & {\sc xml, xslt}, make & Problem notification & Yes\\
Web Server & Any & {\sc ssl} Pages & Yes\\
\hline
\multicolumn{2}{c|}{Ease of use} & \multicolumn{2}{c}{Management}\\
\hline
Drag-N-Drop Content & No & Web Statistics & Yes\\
Email To Discussion & Yes & Content Scheduling & No\\
Friendly {\sc url}s & No & Sub-Sites / Roots & Yes\\
Template Language & Yes & Themes / Skins & No\\
Undo & Yes & Workflow Engine & No\\
{\sc wysiwyg} editor & No & &\\
Macro Language & No & &\\
Server Page Language & No & &\\
\hline
\multicolumn{2}{c|}{Performance} & \multicolumn{2}{c}{Support}\\
\hline
Advanced Caching & No & Online Help & No\\
Database Replication & No & Public Mailing List & Yes\\
Load balancing & No & Pluggable {\sc api} & No\\
Page Caching & Yes & &\\
Static Content Export & Yes & &\\
\hline
\multicolumn{2}{c|}{Interoperability} & \multicolumn{2}{c}{Flexibility}\\
\hline
{\sc rss} Feeds & No & {\sc cgi}-Mode Support & No\\
{\sc ftp} support & No & Content Reuse & Yes\\
{\sc utf-8} support & Yes & Multi-lingual Content & No\\
{\sc xhtml} compliant & Yes & Wiki Aware & No\\
{\sc wai} compliant & Yes & {\sc url} Rewriting & No\\
\hline
\end{tabular}
\end{center}
\caption{Methodology features in a glance}
\label{tbl:cms-matrix}
\end{figure}

\section{Designing a Prototype}

To illustrate our methodology in concrete terms
the following sections contain as a case-study the design and implementation
of our research group's web site.
Through the case study we will demonstrate the key points applied in
practice, discuss important technical and human issues we faced,
and present the lessons we learned.

\subsection{Requirements}
\label{sec:requirements}

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{Diag.eps}
\end{center}
\caption{Overview of data element relationships.}
\label{fig:diag}
\end{figure}

The functional requirements for our center's web site were
simple, but not trivial, and had already been satisfied
twice in two different forms.
The site should present our research center's members, groups, publications,
and projects in an organized fashion.
Our center is divided into five groups.
It numbers about 50 members, is a participant of 35 past and current research
projects, and the originator of about 400 publications.
The site's pages should represent the content and the relationships
we illustrate in Figure \ref{fig:diag}.
The self-referential relationship on the group entity exists because the center should  an ``umbrella'' group of multiple subgroups.
Members of our center and our research projects are associated
with one or more groups.
Publications are associated
with one or more members, groups and projects.
For the sake of simplicity,
we have omitted from our description and the diagram
a number of additional relationships,
such as the member directing a group or managing a project.
As an example of the type of content we were looking for, 
each member, group, or project should have a web page with a list
of the corresponding publications;
each group should have pages listing its projects and members. 
In order to have the ability to add web site data that is not part of the aforementioned categories,
each group can have additional pages of unstructured {\sc xhtml} content, in a dictated manner.
Finally, each member of our groups may be performing a presentation in our group's weekly seminar.

Before embarking on our third attempt,
we articulated those non-functional requirements, to ensure that our 
third attempt would produce
a result with a longer life span.
As we hinted in the previous section, the problem with
the previous implementations was not the creation of the site
satisfying the functional specifications,
but the lack of a number of important non-functional requirements.
The following is a list of the non-functional properties
we deemed important enough to guide our design.

\begin{description}
\item[Openness] The tools used in the realization of the web site
should be available as open source, or supported by multiple vendors.
We wanted to avoid becoming tied with a particular proprietary
tool.
We reasoned that openness would mitigate two risks:
(1) finding a maintainer trained to use a particular proprietary tool,
and (2) obtaining resources for upgrading and maintaining the tool.

\item[Observability]
The semantic distance between
the specification of an element and its implementation 
should be minimal \cite{SG97}.
The site's look and content should be maintainable
using standard tools and techniques.
If possible, the site's maintainer should not be required to
learn a scripting language like {\sc php}, or {\sc perl}, or
a framework like {\sc j2ee} or {\sc .net}.
An approach based on declarative specifications \cite{FFLS00} and
Domain-Specific Languages ({\sc dsl}) \cite{DKV00,Spi00b} would allow end-users, or members
close to end-users to be involved in maintaining the site,
without risking the bottleneck of going through
multiple intermediaries.

\item[Robustness] The web site should not depend on
any external programs other than the web server for serving
its content.
Users updating the data, should be able to author, validate and 
review their changes without requiring network connectivity.
This would allow them to work productively over dial-up connections
or while on the road. In addition, all the editing users should be able 
to work on a platform of their choice ({\sc unix}, {\sc microsoft windows} and {\sc macintosh}) 
using only a text editor of their choice.
Minimizing the dependencies on additional servers (such as a
database or an application server) and on the network
should result in a more robust and easier to maintain system.

\item[Parsimony] The implementation effort for
the system should be minimal.
This would minimize errors and maintenance costs.
We reasoned we could satisfy this requirement by
using existing tools, if their choice satisfied the
other non-functional properties.
\end{description} 

\subsection{Conceptual Framework}

The system's conceptual framework is illustrated in Figure \ref{fig:conc-model}.
The main concept in our system is the {\em Organization}. Each {\em Organization} has 
information that interests {\em Content Consumers}. They 
access the organization's {\em Public Data}, in order to obtain it.
It has also {\em Content Developers} 
who are usually its members. {\em Content Developers} update the 
{\em Public Data} for the {\em Organization}. {\em Content Administrators} 
are supporting the {\em Content Developers} 
(mostly troubleshooting and account management)
and inspecting the submitted {\em Public Data}.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.8]{conc-model}
\end{center}
\caption{Conceptual design of our system}
\label{fig:conc-model}
\end{figure}

\subsection{Motivation and Design Paradigm}
\label{sec:design}

Our main guiding principle was to create a continuous, multi-person 
development activity.
Live web sites continuously evolve;
adopting the content authoring paradigm implied
by our first approach was a mistake. The content of our web site resulted often stale and inconsistent, as a result 
of a single person update process.
Empirical evidence supports this observation.
Yanakoudakis \cite{G03} researched the usability rankings given from 21 Greek government department
web sites between the years 2002 and 2003. He concluded that 
the there is a statically authored pages web sites are degrading to
a point of irrelevance, and then being overhauled from scratch.
The change of ranking can be explained as decay of the web site when dropping,
and re-engineering when rising.

A database-driven approach also hinders evolution.
Changes to the content's presentation require the modification
and installation of the front end page generator;
not a typical lightweight operation.
Changes to the data schema are even more intrusive
requiring a synchronized modification of the data,
the front end, and the back end.

Continuous multi-person based projects are quite
common in software development.
Numerous programmers and engineers contribute and coordinate their work
through a version control system, like {\sc cvs} \cite{BF01} that
maintains a master repository of the source code.
Concepts like the \textit{daily build} \cite{CS95b} or the
\textit{current} and \textit{stable} branches, as practiced by
numerous open source projects, allow the maintenance
of a known-good product.
What we needed for our approach were appropriate,
declarative language-based formalisms for expressing our data,
its transformation into web pages, and an efficient
generation process.

\subsection{Processes}

\begin{figure}[h!]
\includegraphics[scale=0.5]{use-case-diagram}
\caption{{\sc uml} Use Case Diagram of the System}
\label{fig:use-case-diagram}
\end{figure}

Figure \ref{fig:use-case-diagram} shows a {\sc uml} \cite{UML} use case diagram of our system. 
We have three actors (roles) interacting with the system:

\begin{description}
\item[ContentDeveloper] A Content Developer is responsible for uploading data in the system. 
In our case, a Content Developer is typically a single person for each research group. They can upload data files and 
import new bibliography entries in the repository, review and correct any presentation problems and finally publish the 
updated content.

\item[ContentAdministrator] The Content Administrator is responsible for the maintenance 
and the further extension of the system. A Content Administrator corrects problems with the data, develops 
new features or corrects existing ones in the transformation scripts.

\item[ContentConsumer] The content consumers are typically outside of our system
and have access only in the generated web site.
\end{description}

The use cases that comprise our system are:

\begin{figure}
\includegraphics[scale=0.6]{maintain-content-activity}
\caption{{\sc uml} activity diagram for Maintain Content}
\label{fig:maintain-content-diagram}
\end{figure}

\begin{description}
\item[Maintain schemas, transformations and users] This use case describes the 
maintenance and development procedures of the system. Each Content Developer can
update the data schemas and the transformation scripts in order to correct errors 
and add new features.

\item[Update local repository] Content Developers have a local copy of the system's files on their workstation. 
and must update the local repository each time they use the system, in order to synchronize their local copy of the system with the master copy. 
To do so, they must execute an update 
repository command. If there is a conflict, the users must correct the problem 
and rerun the update command.

\item[Commit changes] Content developers and Content Administrators must commit 
the changes from their local repository to the {\sc cvs} repository.

\item[Maintain content] This use case allows Content 
Developers to maintain the content of the web site. 
Content consists of group, member, project and seminar data or bibliography 
collection entries.

\item[Generate web site] Content administrators and developers can generate the 
web site locally for preview. Once the content is generated, 
the user can review the newly integrated content and inspect it
for possible presentation problems.

\item[Validate data] Content developers can validate the data in the local 
repository before the final commit. For the validation process
they use appropriate data schemas.

\item[Manage user accounts] Content Administrators perform user management in the 
system. User management consists of two main processes: 
(1) Authentication , (2) Ownership \& Policy.

\item[Browse web site] This one describes the web site browsing process.

\end{description} 

In Figure \ref{fig:maintain-content-diagram} we show the activity diagram of the 
most complex use case in the system. The Content Developer first updates the 
local repository and then begins data maintenance by adding, 
removing or modifying data files and bibliography entries. Upon completion, data 
validation must be performed before the commit to the data repository. If the data 
repository is valid, then local web site generation must be performed to validate the 
presentation. After that, the data is ready to be submitted to the main 
data repository. After the update of the data repository the process terminates.

\section{Prototype Implementation}

\subsection{Key Technologies}
\label{subsec:key-tech}

Once the design was finalized,
implementation proved to be an almost strange and interesting activity,
since it did not involve almost anything of what
is typically described as coding.

The first step involved selecting and setting up the
appropriate tools. In order to meet the requirements we set in section \ref{sec:requirements}, 
we decided to use popular open
technologies as key elements of our system.
We adopted the concurrent versions system
({\sc cvs}) \cite{BF01,CVS} to coordinate the distribution
and update of all the system's components.
Authentication for managing content was handled by the
Unix group membership mechanism of the host where the
{\sc cvs} repository was installed.
We also used
{\sc BibTeX} \cite{Pa88,Lam94} and {\sc bib2xhtml} \cite{BibXHMTL} for transforming the publications
into {\sc xhtml} and
\textit{xmlstarlet} \cite{Gru04} for validating and transforming
all other {\sc xml}-based data \cite{W3C_XML}.
For data transformations we 
implemented a system based on {\sc xslt} \cite{W3C_XSLT}, a language for transforming {\sc xml} documents.
we chose {\sc xslt} as the transformation language, because it is based on {\sc xml} ({\sc xslt} documents are 100\% valid {\sc xml}), so the
developers can easily learn xslt, if they understand the basics of {\sc xml} technology.
Finally, {\sc gnu} {\em make} \cite{gnu_make} and a couple of shell script
constructs were used for handling the project's makefile.
The complete setup including all tools proved to be portable
between {\sc unix} and {\sc microsoft windows}, with team members working
on machines running different versions of {\sc microsoft windows}, {\sc gnu/linux},
and Free{\sc bsd}.

\subsection{System Development}

The next step was a series of iterations where we
modeled the data's schema on representative {\sc xml}
files. Concurrently, we implemented the validation {\sc dtd / xsd}s (Document Type Definition / X Schemas)
and the transformation {\sc xslt}s. First we developed the {\sc dtd}s
for the data validation, later on we decided to use {\sc xsd} schemas in order to perform
further data validation to our system.
The version control system was already proving its value
at this point
for coordinating the work between the two of the paper's authors.
Because many page elements, like a project's description,
could contain content more elaborate than plain text,
we used {\sc w3c}'s modular {\sc xhtml} specification for
importing existing elements in our {\sc dtd / xsd}s.
This helped us keep our schema description simple,
but the corresponding schema expressive.

\begin{figure}[h!]
\includegraphics[scale=0.6]{generate-web-site}
\caption{{\sc uml} Activity diagram for generating the web site}
\label{fig:generate-web-site}
\end{figure}

In Figure \ref{fig:generate-web-site} we illustrate the web site generation process.
The automated validation and generation of content was
expressed as makefile rules \cite{OTT91}.
The individual {\sc xml} files are merged in a
single {\sc xml} file for cross validating identifier
reference attributes ({\sc idref}s).
The same file is also used to extract the identifiers of
all projects, members, and groups into makefile
variables.
The web pages are assembled through a iteration that applies different perspective each time
to the data. It starts, by generating the group {\sc xhtml} pages, then proceeds with the project, members, groups, seminars, publications.
Each project, member, group etc. refers to the relevant {\sc xml id}s.
Upon generation, links are created dynamically each time, to
connect relevant generated pages e.g., a project is hyperlinked with its corresponding publications,
which are two different independently generated {\sc xhtml} pages.

The {\sc xhtml} content is, by default, generated on the
local machine, where its maintainer can verify it.
After the new content is validated and verified,
the maintainer can commit the change to the central {\sc cvs} repository. 
A separate makefile rule can then be used,
to execute an update command on the
host serving the content to the web.
The command retrieves the updated data from the {\sc cvs}
repository and regenerates the pages on the web-server's
file area.
All components of our system are under version control,
all pages are automatically tagged with identifiers
denoting their source, helping the traceability of changes.
All exchanges between the developers' machines and the
{\sc cvs} and web host are performed using the secure
shell ({\sc ssh}) as the transport protocol guaranteeing the data's integrity
and confidentiality.

\begin{figure}
{\small \tt%
\begin{verbatim}
<seminar>
 <sem_date>20050112</sem_date>
 <sem_time>12:00</sem_time>
 <sem_room>901</sem_room>
 <sem_duration>90 mins</sem_duration>
 <presentation by="m_bkarak">
  <pres_title>Introduction to Regular Expressions</pres_title>
 </presentation>
</seminar>
\end{verbatim}
}
\caption{A typical seminar {\sc xml} file}
\label{fig:project-xml}
\end{figure}

\begin{figure}
{\small \tt%
\begin{verbatim}
<xsl:template match="seminar" mode="full">
 <a name="{current()/sem_date}" />
 <div class="title">
  <xsl:call-template name="date">
   <xsl:with-param name="date" select="sem_date" />
  </xsl:call-template>
 </div>
 <div class="content">
  <h3>Location: <xsl:value-of select="sem_room" /></h3>
  <h3>Time: <xsl:value-of select="sem_time" /></h3>
  <h3>Presentations</h3>
  <xsl:apply-templates select="current()/presentation" mode="full"/>
 </div>
</xsl:template>
\end{verbatim}
}
\caption{The {\sc xslt} transformation file for seminar}
\label{fig:project-xslt}
\end{figure}

\begin{figure}
{\small \tt%
\begin{verbatim}
<a name="20050112"></a>
<div class="title">12 January 2005</div>
<div class="content">
<h3>Location: 901</h3>
<h3>Time: 12:00</h3>
<h3>Presentations</h3>
<table class="content">
<tbody><tr>
<td valign="top"><b>Title:</b></td>
<td><a href="">Introduction to Regular Expressions</a></td>
</tr>
<tr>
<td valign="top"><b>Presented by:</b></td>
<td>
<a href="../members/m_bkarak.html">Mr. Vassilios Karakoidas</a><br />
</td>
</tr>
</tbody></table>
\end{verbatim}
}
\caption{A generated HTML seminar file}
\label{fig:project-html}
\end{figure}

You can see representative samples of a seminar's
{\sc xml} data in Figure \ref{fig:project-xml},
{\sc xslt} transformation in Figure \ref{fig:project-xslt},
and {\sc xhtml} result in Figure \ref{fig:project-html}.

\begin{figure}
\includegraphics[scale=0.8]{distro.eps}
\caption{Directory structure of local repository}
\label{fig:eltrun-web-distro}
\end{figure}

The directory structure of a typical 
local repository is illustrated in Figure \ref{fig:eltrun-web-distro}.
Most of our users are working {\sc microsoft windows} environments and as 
a result we decided to upload a {\sc microsoft windows} version of the needed tools 
in the main repository under the directory \textit{bin}.
The directory \textit{data} contains the {\sc xml} and {\sc BibTeX} files and \textit{schema}
includes all the available {\sc dtd / xsd} and the modular {\sc xhtml} specifications.
The \textit{public\_html} and \textit{build} directory are used for the creation of the 
web site locally, while \textit{doc} contains the available documentation for the Content Developers
and Content Administrators. Finally, the directory \textit{tools} has a small selection 
of utilities ({\sc perl} and shell scripts) that provide statistical information
for our web site.

The installation procedure is very simple,
and the bootstrap tools it requires are only {\sc cvs} for 
the initial check out and an {\sc ssh} client. The needed keys 
for the secure shell session are provided once for each user by the Content Administrator. 
A full version of the system demands 8 megabytes of space 
on the local machine, plus some extra for the local content store and generation.

\begin{figure}
\includegraphics[scale=0.6]{dep-graph.eps}
\caption{Web graph of the generated site}
\label{fig:eltrun-web-m-dds-snapshot}
\end{figure}

In Figure \ref{fig:eltrun-web-m-dds-snapshot} we illustrate a web graph \cite{KRRSTU00} 
that shows references between the {\sc xhtml} pages.
The above Figure shows the references  of a member ("m\_dds" is the {\sc id} for 
Diomidis Spinellis), with other pages in the web site.
The pages starting with "p\_" are projects, with "m\_" members, and "g\_" groups.
At the time of writing the site generated 215 {\sc xhtml} pages with 2432 hyperlinks. 

\section{System Adoption}
\label{sec:adopt}

Our research center is multidisciplinary: under its roof
are both specialized software engineers using the same tools
we adopted in their everyday work, and researchers whose
background is management science, marketing, or finance
who are comfortable with {\sc gui} interfaces.
Upon completion of the development, we were somewhat concerned by the way our group
would receive the new way of work we proposed, in order to maintain the web content.

Our fears were justified.
The first presentation of the system to its users ended
almost in a revolt.
Non-technical users expressed their inability to comprehend
what an {\sc xml} document was, while technical members
helpfully argued for providing a {\sc gui} front end.
By targeting the users with the least technical experience,
promoting our system's ``soft'' attributes,
such as the use of open source software tools,
and convincing them to try to enter a few elements into
the system, we were able to overcome the initial reservations
and start the data migration process.

The next round of problems surfaced when users began entering
malformed or invalid data into the system.
This resulted in all users acquiring the copies of the malformed
{\sc xml} files, and strange error messages given to unsuspecting
users.
As is the custom in a number of development efforts, we had
expected the users to verify the changes they made before
committing them into the {\sc cvs} repository.
Non-technical users were however not aware of this etiquette
and were committing their changes with the hope they were correct.
A shared mailing list had established to explain the
importance of following the correct procedures when committing changes.
We also instituted a ``pointy hat'' policy. Committing a malformed file would award 
its committer a (virtual) ``pointy hat'', which would then be passed on to the next committer to err.
After a few days we got the impression that non-technical users
were becoming confident in their work, even proud of sharing
sophisticated tools and processes with software engineers.
Our technical persons experienced more problems than the inexperienced ones 
and that came as a surprise for us. Some of them were already familiar 
with the key technologies, and they tried to change the tools proposed with others 
more user-friendly. 
These initiatives resulted in corruption of the local repositories, malformed 
{\sc xml} data and wrong bibliography entries.
After a few false attempts they decided to stay with the procedures of the system.

Two weeks after the initial system presentation all users where able to upload 
and maintain their data. The inexperienced users learned how to edit {\sc xml} files,
importing {\sc BibTeX} entries into the system and committing to the {\sc cvs} repository. 
They just followed steps in a procedure that they see as a black box.

A few months from our initial system presentation to our departments technical staff, they decided to adopt our 
methodology, and develop a similar framework to provide technical on-line documentation for our department's students. This development effort started without our involvement, and we think that it's adoption points out the practical value of our approach. We have to note that our technical staff consists of programmers and network administrator, whose experience is limited to technical aspects of computer science.

\section{Lessons Learned}
\label{sec:concl}

We believe that our approach and many of the lessons we learned
can be applied in numerous similar situations,
leading to a lightweight, structured, consistent, and maintainable
web site building method. The proposed design satisfies the non-functional properties
we listed in Section \ref{sec:requirements},
and that our approach stands a higher chance to succeed where the
two other approaches failed.
The initial user reaction was not favorable, but this can
be explained by the significantly higher requirements we
placed on our users. Typical users are not well acquainted
with command line tools, and often see them as a threat to their productivity.
Instead of giving instructions by email to an unfortunate
web site maintainer, they now had to become active members
of an evolving web site maintenance effort.
Not all members of our research center proved ready to take
this responsibility.
Many groups delegated the maintenance to a single person. 
Others started with a centralized approach and later divided the
maintenance responsibilities as they came to appreciate the efficiency
benefits of the distributed site maintenance.
Still, however, we succeeded in distributing the previously
entirely centralized maintenance effort across our groups.
Summarizing, we believe that adopting a software development
metaphor and tools for developing and maintaining semi-dynamic
web site is a practical and worthwhile approach.

\begin{figure}[h!]
\includegraphics[scale=0.6]{cvs-log.eps}
\caption{Commit progress time line}
\label{fig:cvs-log}
\end{figure}

In Figure \ref{fig:cvs-log} we illustrate the {\sc cvs} \textit{commit} commands that have been performed by
the Content Developers and Administrators. Each swimlane in the Figure represents a committing
member of our system. Each horizontal tick represents a single commit instance.

Content Administrators are \textit{dds} (Diomidis Spinellis) and \textit{bkarak} (Vassilios Karakoidas). 
During the initial lit we can see that only the two committed 
changes. It was the development period of the system. After the initial development, a few 
pioneer content developers started to use the system and commit {\sc xml} and bibliography data. In this period 
we also tested the system thoroughly and developed the finalized the presentation of the web site. After the end of
the test period all users became active and began to commit data in a parallel manner. Figure \ref{fig:cvs-log} 
proves that we achieved one of our primary goals, converting the web site maintenance monolithic procedure 
into a distributed over time and multiple user development activity.

\section{Acknowledgments}
\label{sec:ack}

We would like to thank Prof. Manolis Skordalakis for his very perceptive comments during the compilation of this paper.
The authors would also like to thank George Oikonomou and Marianthi Theocharidou who reviewed versions 
of this paper and provided many corrections and observations.

\bibliographystyle{elsart-num}
\bibliography{declweb}

\end{document}
