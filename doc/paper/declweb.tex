% $Id$

\documentclass{article}
\usepackage{mathptmx}
%\usepackage{courier}
\usepackage{graphicx}

\pagestyle{myheadings}
\markboth{Implementing Semi-dynamic Web Sites}{Implementing Semi-dynamic Web Sites}


\begin{document}

\title{A Software Development Metaphor for Implementing Semi-dynamic Web Sites through Declarative Specifications}

\author{Diomidis Spinellis,\footnote{
Contact author.  Address:
Patision 76, GR-106 74, Athens, Greece.
Email:dds@aueb.gr; phone: +302108203981; fax:+302108203370}
\,\,Vassilios Karakoidas, Damianos Chatziantoniou\\
Department of Management Science and Technology\\
Athens University of Economics and Business}

\date{}


\maketitle

\begin{abstract}
Traditionally, the realization of Web sites involves either
static content developed using web authoring tools or dynamic
content delivered by a database-driven front-end,
where the structured content is organized
in a relational schema and dynamically generated.
The limitations of statically-authored web pages are easy to discern, and
for a number of applications, the use of a database
introduces a level of additional complexity that
makes the choice a part of the problem space rather than the solution space.
Based on the distributed software development approach, we present a methodology suitable for managing 
middle-sized semi-dynamic web sites. The technological dimensions of this
approach are well-known technologies and open source tools,
such as {\sc xml} transformations and
version control systems.
The prototype framework we developed for the testing of the proposed methodology
is also demonstrated, along with an overview of our framework acceptance.
\end{abstract}

\subsection*{Keywords}
Web site development,
content management systems,
configuration management,
{\sc xml},
{\sc xslt},
{\sc xsd},
{\sc html}.

\newpage

\section{Introduction}
\label{sec:intro}
Traditionally, the realization of Web sites involves either
static content developed using web authoring tools like
Microsoft's Front Page and DreamWeaver, or dynamic
content delivered by a data-oriented front-end,
where the structured content is stored in a relational database 
and dynamically generated on the fly.
When our group faced a series of problems with both the above approaches,
we decided to explore ideas for a radically different
implementation style, based on the declarative specification
of all the site's elements.

The limitations of statically-authored web pages are easy to discern.
The content is authored in a loosely organized manner with
manually updated web pages, and, as a result,
it can be inconsistent in both structure and presentation.
While the use of cascading style sheets ({\sc css}) can help one obtain a
consistent look, their use still requires discipline.
Moreover, the authored pages remain loosely structured and the resulting
site can be difficult to modify and reorganize. In addition, the information is often duplicated
among hard coded web pages.
Furthermore, the static authoring model often imposes a centralized
management and maintenance style;
all additions and changes have to go through a single person or group,
creating a bottleneck, often leading to outdated content.

Adopting a database driven approach solves
the aforementioned problems.
Separating the source data from its (dynamically generated)
marked-up version ({\sc html} code) leads to a consistent
yet flexible generation of web pages.
In addition, the database's relational model imposes
structure on the data being stored.
Finally, a database back-end allows concurrent updates by
different users.
Note that the adoption of a database-driven approach me be
explicit through the development of a bespoke web application,
or implict through the installation of a general-purpose content
management system.


However, for a number of applications, the use of a database
introduces a level of additional complexity that
makes the choice a part of
the problem space rather than the solution space.
A database-driven web site requires the implementation of a
front-side interface to transform the web site's data into
{\sc html}, and a back-end interface to allow stakeholders
enter, review, and update data.
The back-end client interface typically requires setting up
and maintaining appropriate access permissions.
These may need to be integrated into an organization wide authentication 
facility, or operated under a specific security policy.
In the second case, procedures for setting up passwords,
resetting them, and revoking them need to be established and followed.
A properly running database also requires a skilled database
administrator to install it, maintain it, organize backups,
and perform modifications to the database schema.
In addition, marked-up content is generated by a front-end
program accessing the database, therefore the front-end and the database
must be extremely secure and robust \cite{VG01}, running on a $24 \times 7$ basis.
The front-end, being an executable program working on
untrusted data (the web page requests) can become the target of
malicious attacks,
and must therefore be inspected and audited to ensure its robustness \cite{YHDM04}.
To minimize the risk of an attack against the database
(that would jeopardize the organization's data)
the database server has to be installed on a machine separate
from the web server, behind a properly configured firewall.

Finally, the extraction of content from a database often
induces the web site's designers and stakeholders to adopt a
query-style interface.
Such an interface is typically less usable than browsable web pages,
and the served content is often ignored by search engines,
leading to reduced visibility
of the (meticulously structured) content \cite{DEEP_WEB,JP04}.

All in all, a database-driven approach appears to be suitable
only for those with ample resources to justify the full
development and appropriate maintenance of a sophisticated infrastructure.
Elmasri and Navathe have identified the implications that are created when an organization
adopts such an approach \cite{EN00}. They clarify that an organization should avoid
using a database system, if they cannot afford the overhead cost for:

\begin{itemize}
	\item the high initial investment in hardware, software and training,

	\item the generality of the {\sc dbms} for the definition and processing of data, and

	\item the provision of security, concurrency control, recovery, integrity functions.
\end{itemize}

In addition, they note that it is desirable to use a straight-forward regular file solution when the following criteria are met:

\begin{itemize}
	\item The database and the related applications are simple, well defined and not expected to change

	\item Multiple user access to data is not required
\end{itemize}

Having witnessed the problems we described above in a number of organizations,
we reasoned that a different approach was needed to tackle them.
As an example,
in an internal effort to develop and maintain our group's web site
we had already abandoned the ad-hoc authoring tool--based approach,
because it led to an inconsistent look and stale content,
while the maintenance of a subsequent database-driven
design approach was proving intractable for the resources that
our group could afford.
In the following sections we describe the methodology we developed
for implementing semi-dynamic web sites,
illustrate it by means
of a case study covering the requirements, design, and implementation,
and discuss the lessons we learned.

The main contribution of our work is the identification of
a class of web sites where the application of lightweight
development tools and techniques, such as the ones we describe,
will efficiently yield structured and maintainable content.

\section{Related Work}
\label{sec:related}

During the last decade, the World Wide Web became a popular platform for many {\sc it} applications.
Researchers and companies, in their attempt to make robust web applications,
developed many frameworks and tools to formalize and make efficient the creation and maintenance process.

Fraternali \cite{FRA99} analyzed and studied the perspectives of
web development. He pointed out, that the development process has
five distinct perspectives: (1) {\em process}, (2) {\em models, languages and notation}, (3) {\em reuse}, (4) {\em architecture} and (5) {\em usability}.
These include elements like {\em requirements}, {\em  prototyping and validation}, {\em evolution and maintenance}, that apply in
software engineering scientific areas, and other aesthetic elements like {\em visual quality} and {\em degree of proactivity}.
As we will see in section \ref{sec:requirements}, in our work we took into consideration all the above perspectives and we introduced a few more, like {\em openness}
and {\em observability}.
As a plus, in our methodology we implement the system taking into account the organizational aspect of maintainability of content and the underlying application.

In addition, Fraternali categorizes relevant software tools into six individual categories: (1) {\em visual editors and site managers},
 (2) {\em web-enabled hypermedia authoring tools}, (3) {\em web-{\sc dbpl} (database programming languages) integrators}, (4) {\em web form editors, report writers,
 and database publishing wizards},
 (5) {\em multiparadigm tools} and (6) {\em model-driven application generators} \cite{FRA99}.
 These tools exist in both commercial and open-source incarnations,
and they are widely used.
 In section \ref{subsec:key-tech}, we will see that in our case we chose open source tools to satisfy the {\em openness} non-functional requirement.
 
Many research groups also designed and implemented frameworks to organize the web development process. These frameworks addressed issues
such as {\em lifecycle coverage}, {\em process automation}, {\em modeling abstractions}, {\em reuse and components}. The most popular implementations are 
Araneus \cite{MAM03}, 
Autoweb \cite{FP00} and Strudel \cite{FFLS00}. Many are extended to enhance their abilities. For example, the Strudel
framework introduced a declarative query language, called StruQL that was later extended by FunStruQL \cite{FST99}. Another interesting
framework is WebJinn \cite{KL03}. This framework tries to resolve the crosscutting concerns in web development.

All the above architectures are introducing development methodologies and languages/scriptlets to achieve their goals. For example, Strudel with StruQL
and Araneus with homer. Our goal was to create a system
that would depend on well-known technologies like {\sc xml} and {\sc cvs}.

For a successful high-level design of a web application,
a visual notation is often introduced to specify composition and navigation features in
hypertext applications.
As an example, WebML \cite{CFB00} is a language based on standards like
the entity-relationship model \cite{Che76}
and {\sc uml} \cite{UML}.
Araneus also defined a logical model known as the Araneus Logical Model \cite{MAM03}.
These higher level abstractions are well defined and very useful for the overall design of a web site, but are based on custom,
and sometimes proprietary, technologies.
In our case we based our semantics on {\sc xml} to provide a common data hierarhical organization layer.

Some of the characteristics of our approach have also been implemented by
Jenkins \cite{Jen04} in what he terms
offline programmatic generation of web page, and in the design
of the system S \cite{SCK02}, which uses a
declarative approach for managing data intensive web sites.
Our approach however differs from that of Jenkins
in that we use a declarative specification
rather than imperative code for creating the static content.
Our approach also differs from that of the system S in that
we use standardized and open data formats and tools for the
declarative specification.
On a different front, while Nguyen and his collegues \cite{NMT04}
suggested the use of software engineering configuration management technologies
in the implementation of web projects, they stopped shy of adopting
the corresponding software development toolbench approach we use,
proposing instead a---common in web development---integrated
development environment solution.

Similarities also exist in the context of content management systems ({\sc cms}) \cite{SET02,MT04}
and enterprise content management ({\sc ecm}) \cite{NP04}.
{\sc cms}s support the creation, management, distribution, publishing, and discovery of corporate information, where an {\sc ecm} 
involves a broader range of organizational information. Leading {\sc ecm} providers include Interwoven, 
Vignette, Documentum and others. In those systems, a motivation similar to ours 
leads the research and analogous goals are set \cite{KW05,DOC05}. For example, the unsuitability of static {\sc html} pages for modern 
web sites and the problems of database-driven approach are also mentioned in \cite{KW05}. 
{\sc cms}s based solely on open standards architectures and
{\sc xml} have also been developed,
utilizing methodologies similar to those described here \cite{XYW02,OS05}.
However, in our case,
we do not propose a content management system but an approach for managing content based on established, widely deployed 
and tested software configuration management tools. Our approach can be readily adopted by teams already using {\sc cms}. 
Our use of industry-standard data formats and tools provides us with proven scalability (the same tools have been used 
to manage multi-million line software projects) and flexibility (the tools are used as building blocks and the
freely environment is freely configurable).
The downside of our approach is that many features that come out of the box in {\sc cms}s need 
to be explicitly tailored in our case.

\section{Motivation and Design Paradigm}
\label{sec:design}

Our main guiding principle was to create a continuous, multi-person development activity.
Live web sites continuously evolve;
adopting the content authoring paradigm implied
by our first approach was a mistake.
The resultant content of our web site was often stale and inconsistent,
due to the bottleneck of a single person updating the content.

A database-driven approach also hinders evolution.
Changes to the content's presentation require the modification
and installation of the front end page generator;
not a typical lightweight operation.
Changes to the data schema are even more intrusive
requiring a synchronized modification of the schema,
the data, the front end, and the back end.
The tension between content and data management is lucidly
detailed in an article by Somani {\em et al} \cite{SCK02}.

Continuous multi-person based projects are quite
common in software development.
Numerous programmers and engineers contribute and coordinate their work
through a version control system, like {\sc cvs} \cite{BF01} that
maintains a master repository of the source code.
Concepts like the \textit{daily build} \cite{KAL00} or the
\textit{current} and \textit{stable} branches, as practiced by
numerous open source projects, allow the maintenance
of a known-good product.
What we needed for our approach were appropriate,
declarative language-based formalisms for expressing our data,
its transformation into web pages, and an efficient
generation process.

\section{Methodology}
\label{sec:meth}
Our methodology for developing semi-dynamic web sites uses as
its metaphor the distributed software development activity, as popularized
by many high-visibility distributed open-source development projects,
such as Eclipse \cite{GB04}, Mozilla, and Free{\sc bsd} \cite{Jor01}.
Using a metaphor as a guiding principle for a development activity
is a practice we adopted from extreme programming \cite{Bec00}.
The methodology can be summarized in a few key points.

\begin{itemize}
\item Structure the site's data as {\sc xml} files,
or another declarative formalism.
\item Use {\sc xml} schemas and corresponding tools to validate the content.
\item Exploit modular {\sc xhtml} \cite{W3C_MODULAR_XHTML} to create schemas for rich
data elements.
These can be used to allow the inclusion of specific {\sc xhtml} tags
in the site's {\sc xml} files, without having to reinvent {\sc html}
from scratch.
\item Adopt a version control system for distributing the content,
templates, transformation, and schemas across content developers and
administrators, synchronizing updates, and keeping a record of the project's history.
(Thomas and Hunt \cite{HT00} aptly compare a version control system
with a global undo command with unlimited undo levels.)
\item Create the site's look and feel in {\sc xhtml} by transforming
the data in a declarative way, using languages such as {\sc xslt} and XQuery \cite{HM01,NOV03}.
\item Express verification and building dependencies among
data elements and schemas through {\em make} \cite{MAKE} or {\em ant}
\cite{JAKARTA_ANT} rules.
\item Make the local generation of the site's content the default
building option to allow developers and administrators to verify
the results of  their work, before putting them online.
\item Use existing mechanisms and tools, such as the secure session shell,
public key authentication, and group membership permissions to implement
authentication and authorization policies.
\item Have an instance of the project on a centralized server, kept up to
date through the version control system, as the source to generate and export
the definitive version of the web site's contents.
\end{itemize}

One can easily discern from the above points the similarities of our
approach with distributed software development.
A version control system is used to distribute the artifacts among
developers in their current form, and a building tool, such as {\em make}
or {\em ant} guides the building process.
Developers edit and build their work locally, and commit it to a central server when ready.
The repository on the central server is the source for creating the end-result
of the product, through a---usually---{\em daily build} process.
As we shall later see, in our case the developers work with {\sc xml} and {\sc xslt}
files, instead of programming language source code files, and the build process
is usd extensively as a part of the self-reviewing procedures.

Our proposal also covers the issue of \textit{efficient replication
management}, for almost all the available types of Content Delivery
Networks ({\sc cdn}).
Our version control system centric approach, simplifies the integration
of Server Triggered Replication Strategies \cite{SSPS04}, where the
content can be generated in each replica.
On the other hand, an organization also can apply a static content {\sc
html} replication strategy, since all the web site is generated as plain
{\sc html} pages.

In order to provide a more detailed comparison between our proposed approach and other commercial or open source {\sc cms}s we
created a comparison table based on criteria found in CMSMatrix web site \cite{PBC05}. The feature matrix is illustrated in Figure \ref{tbl:cms-matrix}.

\begin{figure}
\begin{center}
\begin{tabular}{c c | c c}
\hline
\multicolumn{2}{c|}{System Requirements} & \multicolumn{2}{c}{Security}\\
\hline
Application Server & None & Audit Trail & Yes\\
Database & None & Content Approval & Yes\\
License & Free/Open Source & Login History & Yes\\
Operating System & Unix, Win32 & Versioning & Yes\\
Prog/ming Language & {\sc xml, xslt}, make & Problem notification & Yes\\
Web Server & Any & {\sc ssl} Pages & Yes\\
\hline
\multicolumn{2}{c|}{Ease of use} & \multicolumn{2}{c}{Management}\\
\hline
Drag-N-Drop Content & No & Web Statistics & Yes\\
Email To Discussion & Yes & Content Scheduling & No\\
Friendly {\sc url}s & Yes & Sub-Sites / Roots & Yes\\
Template Language & Yes & Themes / Skins & No\\
Undo & Yes & Workflow Engine & No\\
{\sc wysiwyg} editor & No & &\\
Macro Language & No & &\\
Server Page Language & No & &\\
\hline
\multicolumn{2}{c|}{Performance} & \multicolumn{2}{c}{Support}\\
\hline
Advanced Caching & No & Online Help & No\\
Database Replication & No & Public Mailing List & Yes\\
Load balancing & No & Pluggable {\sc api} & No\\
Page Caching & Yes & &\\
Static Content Export & Yes & &\\
\hline
\multicolumn{2}{c|}{Interoperability} & \multicolumn{2}{c}{Flexibility}\\
\hline
{\sc rss} Feeds & No & {\sc cgi}-Mode Support & No\\
{\sc ftp} support & No & Content Reuse & Yes\\
{\sc utf-8} support & Yes & Multi-lingual Content & No\\
{\sc xhtml} compliant & Yes & Wiki Aware & No\\
{\sc wai} compliant & Yes & {\sc url} Rewriting & No\\
\hline
\end{tabular}
\end{center}
\caption{Methodology features at a glance}
\label{tbl:cms-matrix}
\end{figure}

\section{Designing a Prototype}

To illustrate our methodology in concrete terms, 
the following sections contain as a case-study the design and implementation
of our research group's web site.
Through the case study we will demonstrate the key points applied in
practice, discuss important technical and human issues we faced,
and present the lessons we learned.

\subsection{Requirements}
\label{sec:requirements}

\begin{figure}
\begin{center}
\includegraphics[scale=0.7]{Diag.eps}
\end{center}
\caption{Overview of data element relationships.}
\label{fig:diag}
\end{figure}

The functional requirements for our center's web site were
simple, but not trivial.
The site should present our research center's members, groups, publications,
and projects in an organized fashion.
Our center is divided into five groups.
It numbers about 50 members, is a participant of 35 past and current research
projects, and the originator of about 400 publications.
The site's pages should represent the content and the relationships
we illustrate in Figure \ref{fig:diag}.
The self-referential relationship on the group entity exists, because the center should be an ``umbrella'' group of multiple subgroups. Members of our center and our research projects are associated
with one or more groups.
Publications are associated
with one or more members, groups and projects.
For the sake of simplicity,
we have omitted from our description and the diagram
a number of additional relationships,
such as the member directing a group or managing a project.

As an example of the type of content we were looking for, 
each member, group, or project should have a web page with a list
of the corresponding publications;
each group should have pages listing its projects and members. 
In order to have the ability to add web site data that is not part of the aforementioned categories,
each group can have additional pages of unstructured {\sc xhtml} content, in a dictated manner.
Finally, each member of our groups may be performing a presentation in our group's weekly seminar.

Before embarking on our implementation,
we specified a number of non-functional requirements, to ensure that our 
third go would produce a result with a longer life span.
As we hinted in the previous section, the problem with
the previous implementations was not the creation of the site
satisfying the functional specifications,
but the lack of a number of important non-functional requirements.
The following is a list of the non-functional properties
we deemed important enough to guide our design.

\begin{description}
\item[Openness] The tools used in the realization of the web site
should be available as open source, or supported by multiple vendors.
We wanted to avoid becoming tied with a particular proprietary
tool.
We reasoned that openness would mitigate two risks:
(1) finding a maintainer trained to use a particular proprietary tool,
and (2) obtaining resources for upgrading and maintaining the tool.

\item[Observability]
The semantic distance between
the specification of an element and its implementation 
should be minimal \cite{SG97}.
The site's look and content should be maintainable
using standard tools and techniques.
If possible, the site's maintainer should not be required to
learn a scripting language like {\sc php}, or Perl, or
a framework like {\sc j2ee} or {\sc .net}.
An approach based on declarative specifications \cite{FFLS00} and
Domain-Specific Languages ({\sc dsl}) \cite{DKV00,Spi00b} would allow end-users, or members
close to end-users to be involved in maintaining the site,
without risking the bottleneck of going through
multiple intermediaries.

\item[Robustness] The web site should not depend on
any external programs other than the web server for serving
its content.
Users updating the data, should be able to author, validate and 
review their changes without requiring network connectivity.
This would allow them to work productively over dial-up connections
or while on the road. In addition, all the editing users should be able 
to work on a platform of their choice (Unix, Microsoft Windows, or {\sc os-x}) 
using a text editor of their choice.
Minimizing the dependencies on additional servers (such as a
database or an application server) and on the network
should result in a more robust and easier to maintain system.

\item[Parsimony] The implementation effort for
the system should be minimal.
This would minimize errors and maintenance costs.
We reasoned we could satisfy this requirement by
using existing tools, if their choice satisfied the
other non-functional properties.
\end{description} 

\subsection{Conceptual Framework}

The system's conceptual framework is illustrated in Figure \ref{fig:conc-model}.
The main concept in our system is the {\em organization}.
Each organization has information that interests {\em content consumers},
who access the organization's {\em public data}.
The organization houses {\em content developers},
who update its public data.
Finally, {\em content administrators} 
support the content developers
(through training, troubleshooting, and account management)
and also inspect the submitted public data.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.8]{conc-model}
\end{center}
\caption{Conceptual design of our system}
\label{fig:conc-model}
\end{figure}

\subsection{Processes}

\begin{figure}[h!]
\includegraphics[scale=0.5]{use-case-diagram}
\caption{{\sc uml} Use Case Diagram of the System}
\label{fig:use-case-diagram}
\end{figure}

Figure \ref{fig:use-case-diagram} shows a {\sc uml} \cite{UML} use case diagram of our system. 
We have already introduced the actors associated with the system.
The use cases that comprise our system are:

\begin{figure}
\includegraphics[scale=0.6]{maintain-content-activity}
\caption{{\sc uml} activity diagram for Maintain Content}
\label{fig:maintain-content-diagram}
\end{figure}

\begin{description}
\item[Maintain schemas, transformations and users] This use case describes the 
maintenance and development procedures of the system. Each content developer can
update the data schemas and the transformation scripts in order to correct errors 
and add new features.

\item[Update the local repository] Content developers have a local copy of the system's files on their workstation. 
and must update the local repository each time they use the system, in order to synchronize their local copy of the system with the master copy. 
To do so, they must execute an update 
repository command. If there is a conflict, the users must correct the problem 
and rerun the update command.

\item[Commit changes] Content developers and content administrators must commit 
the changes from their local repository to the {\sc cvs} repository.

\item[Maintain content] This use case allows Content 
Developers to maintain the content of the web site. 
Content consists of group, member, project and seminar data or bibliography 
collection entries.

\item[Generate the public web site] Content administrators and developers can generate the 
web site locally for preview. Once the content is generated, 
the user can review the newly integrated content and inspect it
for possible presentation problems.

\item[Validate data] Content developers can validate the data in the local 
repository before the final commit. For the validation process
they use appropriate data schemas.

\item[Manage user accounts] Content administrators also perform user management in the 
system.

\item[Browse web site] This one describes the web site browsing process.

\end{description} 

In Figure \ref{fig:maintain-content-diagram} we show the activity diagram of the 
most complex use case in the system. The content developer first updates the 
local repository and then begins data maintenance by adding, 
removing or modifying data files and bibliography entries. Upon completion, data 
validation must be performed before the commit to the data repository. If the data 
repository is valid, then local web site generation must be performed to validate the 
presentation. After that, the data is ready to be submitted to the main 
data repository. After the update of the data repository the process terminates.

\section{Prototype Implementation}

\subsection{Key Technologies}
\label{subsec:key-tech}

Once the design was finalized,
implementation proved to be an almost ``hollow'' activity,
since it did not involve almost anything of what
is typically described as coding.

The first step involved selecting and setting up the
appropriate tools. In order to meet the requirements we set in section \ref{sec:requirements}, 
we decided to use popular open
technologies as key elements of our system.
We adopted the concurrent versions system
({\sc cvs}) \cite{BF01} to coordinate the distribution
and updating of all the system's components.
Authentication for managing content was handled by the
Unix group membership mechanism of the host where the
{\sc cvs} repository was installed.
We also used
{\sc BibTeX} \cite{Pa88,Lam94} and {\sc bib2xhtml} \cite{BibXHMTL} for transforming the publications
into {\sc xhtml} and
\textit{xmlstarlet} \cite{Gru04} for validating and transforming
all other {\sc xml}-based data \cite{W3C_XML}.
For data transformations we 
implemented a system based on {\sc xslt} \cite{W3C_XSLT}, a language for transforming {\sc xml} documents.
We chose {\sc xslt} as the transformation language, because it is based on {\sc xml} ({\sc xslt} documents are 100\% valid {\sc xml}), so the
developers can easily learn {\sc xslt}, if they understand the basics of {\sc xml} technology.
Finally, {\sc gnu} {\em make} \cite{gnu_make} and a couple of shell script
constructs were used for handling the project's makefile.
The complete setup including all tools proved to be portable
between Unix and Microsoft Windows, with team members working
on machines running different versions of Microsoft Windows, {\sc gnu}/Linux,
and Free{\sc bsd}.

\subsection{System Development}

The next step in our development involved a series of iterations where we
modeled the data's schema on representative {\sc xml}
files. Concurrently, we implemented the validation {\sc dtd / xsd}s (Document Type Definition / X Schemas)
and the transformation {\sc xslt}s. First we developed the {\sc dtd}s
for the data validation, later on we decided to use {\sc xsd} schemas in order to perform
further data validation to our system.
The version control system was already proving its value
at this point
for coordinating the work between the two of the paper's authors.
Because many page elements, like a project's description,
could contain content more elaborate than plain text,
we used {\sc w3c}'s modular {\sc xhtml} specification for
importing existing elements in our {\sc dtd / xsd}s.
This helped us keep our schema description simple,
but the corresponding schema expressive.

\begin{figure}[h!]
\includegraphics[scale=0.6]{generate-web-site}
\caption{{\sc uml} Activity diagram for generating the web site}
\label{fig:generate-web-site}
\end{figure}

In Figure \ref{fig:generate-web-site} we illustrate the web site generation process.
The automated validation and generation of content is
expressed as makefile rules \cite{OTT91}.
The individual {\sc xml} files are merged in a
single {\sc xml} file for cross validating identifier
reference attributes ({\sc idref}s).
The same file is also used to extract the identifiers of
all projects, members, and groups into makefile
variables.
The web pages are assembled through an iteration that applies different perspectives
to the same data.
It starts, by generating the group {\sc xhtml} pages, then proceeds with the projects, members, groups, seminars, and publications.
Each project, member, group etc. refers to the relevant {\sc xml id}s.
Upon generation, links are created dynamically each time, to
connect relevant pages.
For example, a project is hyperlinked with its corresponding publications,
which are two different, independently generated, {\sc xhtml} pages.

The {\sc xhtml} content is, by default, generated on the
local machine, where its maintainer can verify it.
After the new content is validated and verified,
the maintainer can commit the change to the central {\sc cvs} repository. 
A separate makefile rule can then be used,
to execute an update command on the
host serving the content to the web.
The command retrieves the updated data from the {\sc cvs}
repository and regenerates the pages on the web-server's
filesystem.
All components of our system are placed under revision control, and
all pages are automatically tagged with identifiers
denoting their source, helping the traceability of changes.
Furthermore, all exchanges between the developers' machines and the
{\sc cvs} and web host are performed using the secure
shell ({\sc ssh}) as the transport protocol guaranteeing the data's integrity
and confidentiality.

\begin{figure}
{\small \tt%
\begin{verbatim}
<seminar>
 <sem_date>20050112</sem_date>
 <sem_time>12:00</sem_time>
 <sem_room>901</sem_room>
 <sem_duration>90 mins</sem_duration>
 <presentation by="m_bkarak">
  <pres_title>Introduction to Regular Expressions</pres_title>
 </presentation>
</seminar>
\end{verbatim}
}
\caption{An example of the {\sc xml} file describing a seminar}
\label{fig:project-xml}
\end{figure}

\begin{figure}
{\small \tt%
\begin{verbatim}
<xsl:template match="seminar" mode="full">
 <a name="{current()/sem_date}" />
 <div class="title">
  <xsl:call-template name="date">
   <xsl:with-param name="date" select="sem_date" />
  </xsl:call-template>
 </div>
 <div class="content">
  <h3>Location: <xsl:value-of select="sem_room" /></h3>
  <h3>Time: <xsl:value-of select="sem_time" /></h3>
  <h3>Presentations</h3>
  <xsl:apply-templates select="current()/presentation" mode="full"/>
 </div>
</xsl:template>
\end{verbatim}
}
\caption{The {\sc xslt} transformation file for the seminar data}
\label{fig:project-xslt}
\end{figure}

\begin{figure}
{\small \tt%
\begin{verbatim}
<a name="20050112"></a>
<div class="title">12 January 2005</div>
<div class="content">
<h3>Location: 901</h3>
<h3>Time: 12:00</h3>
<h3>Presentations</h3>
<table class="content">
<tbody><tr>
<td valign="top"><b>Title:</b></td>
<td><a href="">Introduction to Regular Expressions</a></td>
</tr>
<tr>
<td valign="top"><b>Presented by:</b></td>
<td>
<a href="../members/m_bkarak.html">Mr. Vassilios Karakoidas</a><br />
</td>
</tr>
</tbody></table>
\end{verbatim}
}
\caption{The generated {\sc html} for the seminar page}
\label{fig:project-html}
\end{figure}

You can see representative samples of a seminar's
{\sc xml} data in Figure \ref{fig:project-xml}, the
{\sc xslt} transformation rules in Figure \ref{fig:project-xslt},
and {\sc xhtml} result in Figure \ref{fig:project-html}.

\begin{figure}
\includegraphics[scale=0.8]{distro.eps}
\caption{Directory structure of local repository}
\label{fig:eltrun-web-distro}
\end{figure}

The directory structure of a typical 
local repository is illustrated in Figure \ref{fig:eltrun-web-distro}.
Many of our users are working on the Microsoft Windows environment, and
therefore the system repository contains a {\sc win32} version of the required
command-line tools under the directory \textit{bin}.
The directory \textit{data} contains the {\sc xml} and {\sc BibTeX} files and \textit{schema}
includes all the available {\sc dtd / xsd} and the modular {\sc xhtml} specifications.
The \textit{public\_html} and \textit{build} directory are used for the creation of the 
web site locally, while \textit{doc} contains the documentation available to
the content developers and content administrators.
Finally, the directory \textit{tools} has a small selection 
of utilities Perl and shell scripts that provide statistical information
for our web site.

The installation procedure is very simple,
and the bootstrap tools it requires are only {\sc cvs} for 
the initial check out and an {\sc ssh} client. The needed keys 
for the secure shell session are provided once for each user by the content administrator. 
A full version of the system requires 8 MB of space 
on the local machine, plus some extra temporary space for the local content generation.

\begin{figure}
\includegraphics[scale=0.6]{dep-graph.eps}
\caption{Web graph of the generated site}
\label{fig:eltrun-web-m-dds-snapshot}
\end{figure}

In Figure \ref{fig:eltrun-web-m-dds-snapshot} we illustrate a web graph \cite{KRRSTU00} 
that shows a subset of references between the {\sc xhtml} pages.
The above Figure shows the references  of a member (``\textit{m\_dds}'' is the {\sc id} for 
Diomidis Spinellis), with other pages in the web site.
The pages starting with ``p\_'' concern projects, with ``m\_'' members, and ``g\_'' groups.
At the time of writing the site contained 249 {\sc xhtml} pages with 3126 hyperlinks. 

\section{System Adoption}
\label{sec:adopt}

Our research center is multidisciplinary: under its roof
are both hard-core software engineers using the same tools
we adopted in their everyday work, and researchers whose
background is management science, marketing, or finance
who are comfortable with graphical user interfaces ({\sc gui}).
Upon completion of the development, we were somewhat concerned by the way our group
would receive the new way of work we proposed to maintain the web content.

Our fears were justified.
The first presentation of the system to its users ended
almost in a revolt.
Non-technical users expressed their inability to comprehend
what an {\sc xml} document was, while technical members
helpfully argued for providing a {\sc gui} front end.
By targeting the users with the least technical experience,
promoting our system's positive ``soft'' attributes,
such as the use of open source software tools,
and convincing them to try to enter a few elements into
the system, we were able to overcome the initial reservations
and start the data migration process.

The next round of problems surfaced when users began entering
malformed or invalid data into the system.
This resulted in all users acquiring the copies of the malformed
{\sc xml} files, and obscure error messages given to unsuspecting
users.
As is the custom in a number of development efforts, we had
expected the users to verify the changes they made before
committing them into the {\sc cvs} repository.
Non-technical users were however not aware of this etiquette
and were committing their changes with the hope they were correct.
We used the mailing list had established to explain the
importance of following the correct procedures when committing changes.
We also instituted a ``pointy hat'' policy. Committing a malformed file would award 
its committer a (virtual) ``pointy hat'', which would then be passed on to the next committer to err.
After a few days we got the impression that non-technical users
were becoming confident in their work, even proud of sharing
sophisticated tools and processes with software engineers.

Two weeks after the first system presentation,
all users where able to upload and maintain their data.
The inexperienced users learned how to edit {\sc xml} files,
importing {\sc BibTeX} entries into the system and committing to the {\sc cvs} repository. 
They just followed a couple of clearly defined step-by-step procedures.

A few months later our department's technical staff, decided to also adopt our 
methodology, and develop a similar framework to provide technical on-line documentation for our undergraduate students.
This development effort started without our involvement, and we think that the
independent adoption of our methodology validated the practicality of our approach.

\section{Conclusions}
\label{sec:concl}

\begin{figure}[h!]
\includegraphics[scale=0.6]{cvs-log.eps}
\caption{Commit progress time line}
\label{fig:cvs-log}
\end{figure}

In Figure \ref{fig:cvs-log} we illustrate the {\sc cvs} \textit{commit} commands that have been performed by
the content developers and administrators. Each swimlane in the Figure represents a committing
member of our system. Each horizontal tick represents a single commit instance.
Content administrators are \textit{dds} and \textit{bkarak}. 
During the initial development period we can see that these were the
only two contributors.
After the initial development, a few 
pioneer content developers started to use the system and commit {\sc xml} and bibliography data. In this period 
we also tested the system thoroughly and developed and finalized the presentation of the web site. After the end of
the test period all users became active and began to commit data in a parallel manner. Figure \ref{fig:cvs-log} 
thus demonstrates that we achieved one of our primary goals, converting the web site maintenance monolithic procedure 
into a distributed over time and multiple user development activity.

We believe that our approach and many of the lessons we learned
can be applied in numerous similar situations,
leading to a lightweight, structured, consistent, and maintainable
web site building method. The proposed design satisfies the non-functional properties
we listed in Section \ref{sec:requirements},
and that our approach stands a higher chance to succeed where the
two other approaches failed.
The initial user reaction was not entirely favorable, but this can
be explained by the significantly higher requirements we
placed on our users. Typical users are not well acquainted
with command line tools, and often see them as a threat to their productivity.
Instead of modifying the site by giving email instructions to an unfortunate
web site maintainer, they now had to become active members
of an evolving web site maintenance effort.
Not all members of our research center proved ready to take
this responsibility.
Many groups delegated the maintenance to a single person. 
Others started with a centralized approach and later divided the
maintenance responsibilities as they came to appreciate the efficiency
benefits of the distributed site maintenance.
Still, however, we succeeded in distributing the previously
entirely centralized maintenance effort across our groups.
In short, we believe that adopting a software development
metaphor and corresponding tools for developing and maintaining semi-dynamic
web site is a practical and worthwhile approach in a number of cases.

\section{Acknowledgments}
\label{sec:ack}

We would like to thank Manolis Skordalakis for his very perceptive comments during the compilation of this paper.
The authors would also like to thank George Oikonomou and Marianthi Theocharidou who reviewed versions 
of this paper and provided many corrections and observations.

\bibliographystyle{alpha}
\bibliography{declweb}

\end{document}
